# Vision Transformer for CIFAR-10 & Multi-View Classification

This project demonstrates a complete MLOps workflow for training Vision Transformer (ViT) models on two distinct image classification tasks: standard CIFAR-10 classification and a simulated multi-view medical classification problem. The project is designed for reproducibility, scalability, and easy deployment, leveraging best practices such as containerization with Docker and automated testing with GitHub Actions.

## Project Overview

The repository contains two main training scripts:

1.  **`src/train_cifar10.py`**: A standard fine-tuning pipeline for a `vit_tiny` model on the CIFAR-10 dataset.
2.  **`src/train_multiview.py`**: A more advanced demonstration of Parameter-Efficient Fine-Tuning (PEFT) using **adapters** for a multi-view classification task. This script uses dummy data and is designed to showcase an innovative, memory-efficient architecture.

## Folder Structure

-   **`.github/`**: Contains GitHub Actions workflows for continuous integration.
-   **`configs/`**: Stores YAML files (`config.yaml` and `multiview_config.yaml`) for managing hyperparameters.
-   **`src/`**: The main source code directory, including both training scripts.
-   **`Dockerfile`**: Defines the Docker environment for a consistent runtime.
-   **`requirements.txt`**: Lists all necessary Python dependencies.

## Setup and Installation

### Prerequisites

-   Docker installed on your system.
-   Python 3.8+
-   A NVIDIA GPU with CUDA drivers (for GPU support).

### Local Environment Setup

1.  **Navigate to your projects directory:**
    ```bash
    cd /path/to/MLPractice/GithubProjects/your_repo
    ```

2.  **Activate your virtual environment:**
    ```bash
    source ../sklearn-pytorch-env-py312/bin/activate  # On Windows, use `..\\sklearn-pytorch-env-py312\\Scripts\\activate`
    ```
    
### Generating `requirements.txt`

The `requirements.txt` file can be automatically generated by scanning your source code for imported libraries. This ensures your dependency list is always accurate and minimal.

1.  **Install `pipreqs`:**
    ```bash
    pip install pipreqs
    ```

2.  **Generate the file:**
    ```bash
    pipreqs . --force
    ```

This command will create a `requirements.txt` file at the root of your project, listing only the libraries your code directly imports.

3.  **Install dependencies from the generated file:**
    ```bash
    pip install -r requirements.txt
    ```

## Training the Models

You can run each training script from the command line, referencing its respective configuration file.

### For CIFAR-10 Classification:

```bash
python src/train_cifar10.py
```

### For Multi-View Classification with Adapters:

```bash
python src/train_multiview.py
```


### Build the Docker image
```bash
docker build -t vit-mlops-project:latest .
```

To run a specific training script inside the container, you need to override the default command. You must also add the --shm-size flag to allocate enough shared memory for parallel data loading.

Understanding num_workers and Shared Memory

Your scripts are configured to use multiple num_workers for the data loader. This helps speed up training by loading data in parallel using your CPU while the GPU is busy with computations. However, these workers require a dedicated amount of shared memory (/dev/shm) to communicate. Docker's default shared memory size is often too small for this, leading to an OverflowError. By adding --shm-size=2g to the docker run command, you allocate a sufficient amount of memory to prevent this error and ensure the data loading remains fast.

### Run the CIFAR-10 training script inside the container with GPU support
```bash
# We add `--shm-size=2g` to allocate enough shared memory for data loading.
docker run --gpus all --shm-size=2g --name vit-cifar10-trainer vit-mlops-project:latest python3.10 src/train_cifar10.py

```
### Run the multi-view training script inside the container with GPU support
```bash
docker run --gpus all --shm-size=2g --name vit-cifar10-trainer vit-mlops-project:latest python3.10 src/train_multiview.py
```